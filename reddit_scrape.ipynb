{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shafee Syed-Quadri, Karthik Thiagarajan, Ammar Mustufa, \n",
    "Tom Starkie, Nawen Deng, Geer Zhang, Haoxiang Yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting prawcore<3,>=2.1 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update-checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.11.17)\n",
      "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update-checker, prawcore, praw\n",
      "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.7.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\86153\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from requests import Session\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: wucanyue1\n"
     ]
    }
   ],
   "source": [
    "# Check if the authentication is successful by accessing the current Redditor\n",
    "try:\n",
    "    print(f\"Authenticated as: {reddit.user.me()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to authenticate: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need a rant about my gf \n",
      "29\n",
      "45\n",
      "/r/mentalhealth/comments/1fzx6nt/i_need_a_rant_about_my_gf/\n",
      "Do not sacrifice your mental health and take verbal and emotional abuse just to not be alone. I will tell you tat does not end well 13 years for me and I was in psychiatric care 35 times for what it all did to me. It;s on HER and HER alone to handle her problems and mental health.\n",
      "Key_Flounder_7149\n",
      "U need drop her like yesterday.\n",
      "At your age, u have your own mental and emotional development to focus on (I‚Äôm 46 M).\n",
      "\n",
      "U don‚Äôt know who u are yet, your confidence will build with age.  No BS, it‚Äôs hard for men these days, and yeah you may need to put yourself out there again, but will build your character.\n",
      "\n",
      "The last thing you should do is cling on to a toxic partner.\n",
      "BigChemistry1322\n",
      "You‚Äôre 16 chances are you‚Äôre not going to end up with her. Time to cut your loses and move on. If she does this now just imagine how much worse it‚Äôll get down the line. Move on dude and fine someone better that respects you.\n",
      "alexrios86\n",
      "Your girlfriend is being disrespectful and toxic. I am sorry you are experiencing this. You need to have a serious conversation with her about the way it makes you feel when she says those things. Communication is key in any relationship. Her behavior is not okay. If she doesn‚Äôt change, it may be best to end the relationship. You deserve respect, reciprocation, and kindness.\n",
      "icollectsquish\n",
      "You need to learn to say \"I'm not okay with the way you're talking to me right now\". \n",
      "And tell her you won't keep talking to her if she keeps treating you like that\n",
      "noodleworm\n",
      "Shes abusing you. Break it off. I wish I had. Preferably do it around people so you have witnesses in case she tries something. Please DO NOT stay.\n",
      "SeparateRanger330\n",
      "My boyfriend and I are the ages of you and your gf's. I showed this to him and we wanna remind you that:\n",
      "\n",
      "1. Your feelings are valid. What right does anyone have to prioritize another person's wellbeing over their own?\n",
      "\n",
      "2. It can be difficult but it's important to take care of yourself and address stress like this, given its impact on your mental health and personal life.\n",
      "\n",
      "3. We are only so young. We owe it to ourselves to live life the best we can while focusing on stuff that truly matter. Why deal with such negativity? You don't deserve this, buddy. You deserve a caring and respectful partner. Remind yourself of your worth and live out your youth to the fullest. Be as happy as you wish and deserve.\n",
      "\n",
      "4. Your bravery in acknowledging the fear of loneliness is admirable. There is not one existing person without this fear. We have it as well. The reality is that we are all individuals incapable of constant dependence on one another. We all need to find comfort in our independence eventually. Becoming comfortable and finding joy in your own company helps with that. You are the best friend you'll ever have in life because you will always be there. You can always put yourself first. Who says loneliness has to be so bad? Anyway, you are sixteen. You're granted the great opportunity of growing into self-care and self-love at a young age. At the right time and place, you will find the right person for you. We promise you that person is not someone who'd do such horrible things to you.\n",
      "\n",
      "Wishing you the best of luck in all of your future endeavours! Stay strong, you got this\n",
      "34mitch12\n",
      "you shouldn‚Äôt have to take that, that‚Äôs really unfair that she lets out those words and actions onto you, treating you like an emotional punching bag and not even caring to ask about how you feel on things. I‚Äôve been like this (your gf) in my most recent relationship (who has recently broken up) and i can‚Äôt imagine how exhausting that is, I‚Äôm sorry you have to go through that :( I‚Äôd suggest maybe (if you haven‚Äôt already) communicating how upsetting you find these lashouts to be, and that you feel upset she never asks about how you feel on things? but also don‚Äôt feel you have to stay in this relationship, it could eventually lead to resentment and may end up in a real lash out on your end, or may cause your mental health to go on a decline. I understand the fear of being alone and that you feel you may regret leaving her due to her good traits, but it‚Äôs not worth sacrificing yourself for if it‚Äôs causing so much anger and pain. she has some stuff to work on by the sounds of things and hopefully she can realise that.\n",
      "Full-Rutabaga9881\n",
      "Your in high school man, don't waste 1 more second on someone like this. You have your whole life to deal with adult problems, you need to be concentrating on things that bring you joy and enjoying being a kid because these are some of the best days of your life. Don't get caught up in negativity over some relationship that more than likely won't last.\n",
      "Amos_Burton666\n",
      "man relationships can be tough especially when both sides are going through stuff. communication is key for sure. maybe try talking it out and setting some boundaries. everyone has bad days but mutual respect is important. just remember you gotta take care of yourself too.\n",
      "xcharm_nal\n",
      "Sorry to break it to you, but being in a relationship to assuage your fear of loneliness is not healthy. You have to be comfortable being alone to be in a healthy relationship, because you and your partner are not the same person, and can't always be by each others side.\n",
      "Internal_Struggles\n",
      "You're 16. Do yourself a favor and move on.\n",
      "MaverickFischer\n",
      "I once dated someone like this when I was 16. I left the relationship with full blown psychosis from mental abuse eventually. Do yourself a favour and move on. \n",
      "One rule I made for myself is to always have self respect in a relationship. You shouldn't be worried or sad.  You should be angry that you gave someone love and they thanked you with insults.\n",
      "Cum_on_a_cactus\n",
      "As an adult who was like this to my partner when I was 15, seriously you don‚Äôt need to stay with her. It might seem like her world will end and you might feel responsible for how it makes her feel but her feelings aren‚Äôt your burden. You‚Äôre young. Enjoy it. When my boyfriend at the time left me, it sucked for me, but him explaining why my behaviors were hurtful lead me to being a better person, and I figured out how to handle my issues on my own. She‚Äôs responsible for herself. Just wanted to give you another perspective. Take care of yourself and wish her the best. Good luck üëçüçÄ\n",
      "Pinnete\n",
      "Being alone is better than being with bad company, and people will take advantage of those that fear being alone. Nothing better than having a back bone and being aware that youre worth more than dealing with peoples dumb unnecessary bullshit, we all have our problems. People will respect you more if they see that you can stand tall alone and wont hang around bullshit they dont need. But im sure you‚Äôll awaken to this the older you get, comes with maturity. Just dont get anyone pregnant while youre still ‚Äú young, dumb, wild and free ‚Äú üòÇ\n",
      "ArtHungry1902\n",
      "Drop her. She‚Äôs not gonna be your soulmate anyways. She‚Äôs just a waste of time.\n",
      "Exh4ustedXyc\n",
      "It sounds like a mental health issue.  As someone with bipolar it could be that.  At that age, though, I don't think I'd put myself that if I were you.  Unless she's really willing to seek help and work on herself and hear you out on your needs and put in the effort, you don't need that.  I have been working on myself since my teen years and even I have had trouble.  It's a lot to deal with.   Better to not waste the energy at that age.   You won't be alone forever.\n",
      "Miss_Management\n",
      "If you have time walk on eggshells and mentally prepare for her daily ,it's time to wrap it up. It's gonna be hard and it's gonna hurt but your mental health and being treated decently is more important. You are still so so young and you'll find someone sooner than you know it who will treat you so much better.\n",
      "BobaMoon\n",
      "Brother you‚Äôre only 16.\n",
      "\n",
      "You don‚Äôt need that. \n",
      "\n",
      "You have your whole life ahead of you and you can meet so many other girls who won‚Äôt treat you like that.\n",
      "theguill0tine\n",
      "Bro. You‚Äôre 16. Scared of being alone again‚Ä¶? You have YEARS. This is the most ridiculous mindset. Fella, she needs therapy and you need to get out of that relationship, and fix your attachment issues before college. \n",
      "\n",
      "End it, get into therapy yourself and ask why at the age of 16, why you‚Äôre scared of being alone forever. Or again. I‚Äôm sure you‚Äôll probably have another gf or partner in a year or two. You need to figure yourself out first, dawg. You‚Äôre a kid. You got this. I promise.\n",
      "\n",
      "I was an angry 15 year old and I can promise you, sometimes it doesn‚Äôt work out to show you what you can and can‚Äôt handle. Maybe you won‚Äôt settle down for another 20 years! Your teens and 20s is meant to figure yourself out and what you actually want out of life for YOU. You will enter a lot of relationships before finding what you do want for the rest of your life. You may get married and get divorced one day and you look back and chuckle at this. \n",
      "\n",
      "You‚Äôre good. You‚Äôre young. You have so much time. I‚Äôm 23 now and felt like I would be alone forever once I broke up with my boyfriend at 15/16. In a few weeks? Eh, I was on the soccer team and had finals. \n",
      "\n",
      "When you put it this way, it‚Äôs a bit silly huh? All love. You‚Äôre smart and you know what to do. ü§ç\n",
      "\n",
      "Edit to add: the toughest lesson in dating at this age is understanding that these relationships probably won‚Äôt last. Also, a tough pill to swallow even for me in my last relationship at 21, was that you cannot change anyone. You want her to change, and that‚Äôs okay, but staying with someone for hope of potential will damage you. In that case, take every moment you can with a lesson and a grain of clarity afterwards. You can‚Äôt change anyone, you can only change the way you play a role in that persons life. Do you seriously like her very much or just don‚Äôt wanna be alone? That‚Äôs an attachment question and a great journal prompt. Start there.\n",
      "Accomplished_Jello66\n",
      "Definitely agree with everyone else to break up with her! Either way you're in a difficult situation: stuck with a toxic girlfriend or alone. But which is worse? Probably the toxic girlfriend. She could affect your own self esteem and mental health which not good at all and the longer you're with her, the longer you won't be able to find someone who actually values you and that you enjoy being with. Good luck!\n",
      "cherrie_e\n",
      "You‚Äôre way too young to be staying in an ABUSIVE relationship just to stay not-single.\n",
      "SanguineElora\n",
      "A lesson to learn early in dating as possible: if a person verbally abuses and insults you, they don't like you. Don't date people who are jerks and don't like you. I'm a woman and I approve this message. \n",
      "\n",
      "Dump her!\n",
      "StaticCloud\n",
      "Hey, I just glanced at your profile and it sounds like the two of you really need to be focusing on yourselves and not each other. She clearly needs support - but so do you. At 15 and 16, it‚Äôs neither of your jobs to play that part. You can‚Äôt save someone else if you aren‚Äôt saving yourself. It‚Äôs that whole putting your own oxygen mask on first before someone else thing. Do you have a school counselor you can talk with?\n",
      "AnxiousElixr87\n",
      "Are you truly alone store to not having a girlfriend? Don't you have friends? Is toxicity worth it just for the sake of company?\n",
      "fanime34\n",
      "If she is making you depressed tho, just dump her. After that she will see how much she needs you, and if not she never gave a shit about you in the first place and is a toxic person out of your life. \n",
      "\n",
      "Trust me Ik what I‚Äôm talking about.\n",
      "Starwarsnerd9BBY\n",
      "I‚Äôm sorry, young friand. You‚Äôve gotta look out for yourself and believe me, you‚Äôll never feel more alone in your life than being in a relationship with someone like that. Hugs ü§ó You‚Äôll get through it, keep close friends and kind people around you to help!\n",
      "thillythillygoose\n",
      "And if she doesn‚Äôt ask from genuine concern now it‚Äôs not a problem you should perseverate on. \n",
      "\n",
      "As soon as you identify a communication barrier assuming you can‚Äôt easily fix it you‚Äôre already in an uphill battle.\n",
      "\n",
      "It takes great maturity to focus on someone else‚Äôs wants and needs especially in high school. Perhaps you have different expectations of one another in which case you must alert her of there importance and if she‚Äôs still dismissive you may have to start thinking about dating someone else\n",
      "Unfathomable-swag\n",
      "If your best friend told you this same story, what would you tell him to do?\n",
      "chillyHill\n",
      "Avoid borderline people if you want to be happy\n",
      "IllustriousMight6\n",
      "I'm sorry, but you're already alone if you're in a relationship like that. Your mental health matters too, and it shouldn't be all about her. Plus you don't want to deal with this for the rest of your life, so either take a step back or have a talk with her. Don't think you have to put up with that kind of treatment.\n",
      "DryWater459\n",
      "Being alone is better than being in bad company. In your situation, you're better off journaling or talking to the wall because she doesn't listen anyways\n",
      "R34L17Y-\n",
      "don‚Äôt waste your development years dreading cause of someone that decides to be a dick. drop her\n",
      "marsloon\n",
      "dude same thing happens to me and ik the same situation ur in. trust me things will get better if u leave ik it seems like u can‚Äôt but u have to\n",
      "Potential-Bat1861\n",
      "Commmunicate boundaries and express feelings!\n",
      "No-Yogurt3348\n",
      "Anyone, regardless of age, who lashes out and speaks to you like that repeatedly, is displaying extremely toxic behaviour. \n",
      "\n",
      "Honestly, you guys are so young (I know how annoying that must sound). I wouldn't waste my time with that. You will find someone very very quickly. The benefit of ending this is you are saving your mental health in the long run and storing this experience as things to avoid in the future. \n",
      "\n",
      "Consider ending things if you are honestly not happy. End things or ask for a break. Go no contact for about 2 to 3 weeks. You will start to feel better or at least have more clarity about what to do next.\n",
      "bunnyhugbandit\n",
      "This is a toxic and emotionally abusive relationship. Someone who treats you like shit then expects you to wait on them hand and foot, while giving nothing back, does not deserve your time. Tell her in person, calmly but firmly, that you don't deserve to be treated like this and that you are ending the relationship.\n",
      "justpassingluke\n",
      "Shed not the one, end it.\n",
      "Feeling_the_Mode\n",
      "Being alone won‚Äôt cause you this type of pain. I‚Äôm alone and I‚Äôve always been. I‚Äôm so okay with feeling lonely than to feel neglected and hurt by someone who treats me like garbage. Think about it, this relationship isn‚Äôt good for your mental health.\n",
      "Altruistic_Rhubarb68\n",
      "sounds like she needs therapy, I was varying from screaming at people, crying on the floor, hitting walls and lampposts getting bloody knuckles and severe nerve damage before I was on full dose of lamotragine, now I am doing better. if your girlfriend is causing you emotional distress leave her and suggest to her that maybe she needs therapy.\n",
      "forestlady4\n",
      "You already are alone, man. That doesn't sound like a relationship. Look for better.\n",
      "Fishnets00\n",
      "                    user                                            comment\n",
      "0      Key_Flounder_7149  Do not sacrifice your mental health and take v...\n",
      "1       BigChemistry1322  U need drop her like yesterday.\\nAt your age, ...\n",
      "2             alexrios86  You‚Äôre 16 chances are you‚Äôre not going to end ...\n",
      "3         icollectsquish  Your girlfriend is being disrespectful and tox...\n",
      "4             noodleworm  You need to learn to say \"I'm not okay with th...\n",
      "5      SeparateRanger330  Shes abusing you. Break it off. I wish I had. ...\n",
      "6              34mitch12  My boyfriend and I are the ages of you and you...\n",
      "7      Full-Rutabaga9881  you shouldn‚Äôt have to take that, that‚Äôs really...\n",
      "8         Amos_Burton666  Your in high school man, don't waste 1 more se...\n",
      "9             xcharm_nal  man relationships can be tough especially when...\n",
      "10    Internal_Struggles  Sorry to break it to you, but being in a relat...\n",
      "11       MaverickFischer        You're 16. Do yourself a favor and move on.\n",
      "12       Cum_on_a_cactus  I once dated someone like this when I was 16. ...\n",
      "13               Pinnete  As an adult who was like this to my partner wh...\n",
      "14         ArtHungry1902  Being alone is better than being with bad comp...\n",
      "15          Exh4ustedXyc  Drop her. She‚Äôs not gonna be your soulmate any...\n",
      "16       Miss_Management  It sounds like a mental health issue.  As some...\n",
      "17              BobaMoon  If you have time walk on eggshells and mentall...\n",
      "18         theguill0tine  Brother you‚Äôre only 16.\\n\\nYou don‚Äôt need that...\n",
      "19  Accomplished_Jello66  Bro. You‚Äôre 16. Scared of being alone again‚Ä¶? ...\n",
      "20             cherrie_e  Definitely agree with everyone else to break u...\n",
      "21         SanguineElora  You‚Äôre way too young to be staying in an ABUSI...\n",
      "22           StaticCloud  A lesson to learn early in dating as possible:...\n",
      "23        AnxiousElixr87  Hey, I just glanced at your profile and it sou...\n",
      "24              fanime34  Are you truly alone store to not having a girl...\n",
      "25      Starwarsnerd9BBY  If she is making you depressed tho, just dump ...\n",
      "26     thillythillygoose  I‚Äôm sorry, young friand. You‚Äôve gotta look out...\n",
      "27     Unfathomable-swag  And if she doesn‚Äôt ask from genuine concern no...\n",
      "28            chillyHill  If your best friend told you this same story, ...\n",
      "29     IllustriousMight6    Avoid borderline people if you want to be happy\n",
      "30           DryWater459  I'm sorry, but you're already alone if you're ...\n",
      "31              R34L17Y-  Being alone is better than being in bad compan...\n",
      "32              marsloon  don‚Äôt waste your development years dreading ca...\n",
      "33     Potential-Bat1861  dude same thing happens to me and ik the same ...\n",
      "34         No-Yogurt3348      Commmunicate boundaries and express feelings!\n",
      "35        bunnyhugbandit  Anyone, regardless of age, who lashes out and ...\n",
      "36       justpassingluke  This is a toxic and emotionally abusive relati...\n",
      "37      Feeling_the_Mode                          Shed not the one, end it.\n",
      "38  Altruistic_Rhubarb68  Being alone won‚Äôt cause you this type of pain....\n",
      "39           forestlady4  sounds like she needs therapy, I was varying f...\n",
      "40            Fishnets00  You already are alone, man. That doesn't sound...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/mentalhealth/comments/1fzx6nt/i_need_a_rant_about_my_gf/\"\n",
    "\n",
    "submission = reddit.submission(url=url)\n",
    "\n",
    "print(submission.title)\n",
    "print(submission.score)\n",
    "print(submission.num_comments)\n",
    "print(submission.permalink)\n",
    "\n",
    "# get all comments and who posted them\n",
    "for comment in submission.comments:\n",
    "    print(comment.body)\n",
    "    print(comment.author)\n",
    "\n",
    "# for each comment, get user name and storce in dataframe\n",
    "comments = []\n",
    "for comment in submission.comments:\n",
    "    comments.append({'user': comment.author, 'comment': comment.body})\n",
    "\n",
    "df = pd.DataFrame(comments)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "0    New wiki on how to avoid accidentally encourag...   \n",
      "1    Please remember that NO ACTIVISM of any kind i...   \n",
      "2                   I'm bleeding out on a park bench.    \n",
      "3    Hiding information about how to kill oneself s...   \n",
      "4    I am no longer enjoying life. If you are no lo...   \n",
      "..                                                 ...   \n",
      "944                                      I am a burden   \n",
      "945                                Really struggling.    \n",
      "946  Feels like the world is ending every day, alre...   \n",
      "947  if it wasn‚Äôt for my belief in God I‚Äôd be dead ...   \n",
      "948                             Grappling with forever   \n",
      "\n",
      "                                                  body              author  \n",
      "0    We've been seeing a worrying increase in pro-s...            SQLwitch  \n",
      "1    Activism, i.e. advocating or fundraising for s...            SQLwitch  \n",
      "2    There's an ambulance that keeps going up and d...  Rude-Geologist5914  \n",
      "3    I want to kill myself. My life is ruined and I...           MarcJAMBA  \n",
      "4    There is not a single thing I enjoy doing anym...    StageMelodic2680  \n",
      "..                                                 ...                 ...  \n",
      "944  I can't wait to finally give my parents a brea...              emxomo  \n",
      "945  I feel like there is no point anymore. My ment...             deleted  \n",
      "946  The world is fucking scary, man. I'm young, an...       jummy-parvati  \n",
      "947  I‚Äôm 18 and I‚Äôm so exhausted of going in circle...     Bitter-Life2116  \n",
      "948  Growing up I struggled with a lot of things. W...    TomatoMaster8158  \n",
      "\n",
      "[949 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fetch 5000 posts from the subreddit\n",
    "url = \"https://www.reddit.com/r/suicidewatch/\"\n",
    "subreddit = reddit.subreddit(\"suicidewatch\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Loop through each post and gather the required details\n",
    "for submission in subreddit.hot(limit=2000):\n",
    "    posts.append({\n",
    "        'title': submission.title,        # Title of the post\n",
    "        'body': submission.selftext,      # Body (text) of the post\n",
    "        'author': submission.author.name if submission.author else 'deleted'  # Author of the post\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the collected data\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('suicidewatch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 997/1589 posts collected (62.74% complete)\n",
      "Progress: 1589/1589 posts collected (100.00% complete)\n",
      "Progress: 1590/1589 posts collected (100.06% complete)\n",
      "Progress: 1591/1589 posts collected (100.13% complete)\n",
      "Progress: 1592/1589 posts collected (100.19% complete)\n",
      "                                                  title  \\\n",
      "0              r/MentalHealth is looking for moderators   \n",
      "1                            Politics and Mental Health   \n",
      "2     Be brutal, what would make you leave your frie...   \n",
      "3     We'll be completely forgotten after a few gene...   \n",
      "4                      My Place Is Right By Your Side üíï   \n",
      "...                                                 ...   \n",
      "994   How Does Childhood Abandonment Trauma Affect T...   \n",
      "995                                         I love youü•∫   \n",
      "996                  I'm going insane I hate everything   \n",
      "1589  Mental Health Awareness Month: I have schizoaf...   \n",
      "1590       my boyfriend k*lled himself in my apartment    \n",
      "\n",
      "                                                   body                author  \n",
      "0     Hey r/mentalhealth! We're looking to grow our ...       DrivesInCircles  \n",
      "1     Hello friends!\\n\\nThe team has noticed an incr...                  Pi25  \n",
      "2     I'm sorry if this sounds rude, it's not my int...                unsw4g  \n",
      "3     I am trying to contemplate the meaning of life...  ButterscotchPure6436  \n",
      "4     If you‚Äôre feeling alone, fighting a battle rag...           happy_neets  \n",
      "...                                                 ...                   ...  \n",
      "994   Hi everyone, I‚Äôve been thinking a lot about th...          AuthorOk4016  \n",
      "995   Today is one of those days that ended well and...           happy_neets  \n",
      "996   I feel like I have no control over my mind any...            beanzybean  \n",
      "1589                                                          WarmlyEccentric  \n",
      "1590  my boyfriend and i got into a bad fight where ...  Small_Nectarine_8856  \n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from prawcore.exceptions import RequestException, ResponseException\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 2000\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "    except (RequestException, ResponseException) as e:\n",
    "        print(f\"Hit rate limit or request error: {e}. Waiting 5 minutes before retrying...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id  # Add post ID to help identify duplicates\n",
    "        })\n",
    "        if len(posts) >= desired_post_count:\n",
    "            break  # Stop once we reach the desired post count\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "        if len(posts) >= desired_post_count:\n",
    "            break\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "        if len(posts) >= desired_post_count:\n",
    "            break\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "        if len(posts) >= desired_post_count:\n",
    "            break\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "        if len(posts) >= desired_post_count:\n",
    "            break\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mentalhealth4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 996/1417 posts collected (70.29% complete)\n",
      "Progress: 1989/1417 posts collected (140.37% complete)\n",
      "Progress: 2982/1417 posts collected (210.44% complete)\n",
      "Progress: 3982/1417 posts collected (281.02% complete)\n",
      "Progress: 4977/1417 posts collected (351.24% complete)\n",
      "Progress: 5975/1417 posts collected (421.67% complete)\n",
      "Progress: 6257/1417 posts collected (441.57% complete)\n",
      "                                                  title  \\\n",
      "0              r/MentalHealth is looking for moderators   \n",
      "1                            Politics and Mental Health   \n",
      "2     Be brutal, what would make you leave your frie...   \n",
      "3     We'll be completely forgotten after a few gene...   \n",
      "4                      My Place Is Right By Your Side üíï   \n",
      "...                                                 ...   \n",
      "5641  Suggestions for self care while helping an eld...   \n",
      "5642                       Need help understanding this   \n",
      "5643                      Struggling with friend group    \n",
      "5644  I haven‚Äôt been feeling like myself lately. Wha...   \n",
      "5645                                    laughing people   \n",
      "\n",
      "                                                   body                author  \n",
      "0     Hey r/mentalhealth! We're looking to grow our ...       DrivesInCircles  \n",
      "1     Hello friends!\\n\\nThe team has noticed an incr...                  Pi25  \n",
      "2     I'm sorry if this sounds rude, it's not my int...                unsw4g  \n",
      "3     I am trying to contemplate the meaning of life...  ButterscotchPure6436  \n",
      "4     If you‚Äôre feeling alone, fighting a battle rag...           happy_neets  \n",
      "...                                                 ...                   ...  \n",
      "5641  My father was diagnosed with Parkinson‚Äôs and D...              krenchra  \n",
      "5642  TW: sexual abuse, underage\\n\\nSexual assault i...         Minute_Ad2297  \n",
      "5643  Was wondering if anyone could offer advice sin...              MarshBaz  \n",
      "5644  Lately, nothing I do feels right. I think it‚Äôs...                aradoc  \n",
      "5645  I see people laughing at me in my head when I ...      princessjayla222  \n",
      "\n",
      "[3803 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from prawcore.exceptions import RequestException, ResponseException\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 1000\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "    except (RequestException, ResponseException) as e:\n",
    "        print(f\"Hit rate limit or request error: {e}. Waiting 5 minutes before retrying...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id  # Add post ID to help identify duplicates\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this week'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='week'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'today'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='day'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mentalhealth.csv4', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Temp\\ipykernel_27340\\1786485583.py:47: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 996/1414 posts collected (70.44% complete)\n",
      "Progress: 1989/1414 posts collected (140.66% complete)\n",
      "Progress: 2982/1414 posts collected (210.89% complete)\n",
      "Progress: 3982/1414 posts collected (281.61% complete)\n",
      "Progress: 4976/1414 posts collected (351.91% complete)\n",
      "Progress: 5976/1414 posts collected (422.63% complete)\n",
      "Progress: 6249/1414 posts collected (441.94% complete)\n",
      "Progress: 6274/1414 posts collected (443.71% complete)\n",
      "Progress: 7267/1414 posts collected (513.93% complete)\n",
      "Progress: 7292/1414 posts collected (515.70% complete)\n",
      "                                                  title  \\\n",
      "0              r/MentalHealth is looking for moderators   \n",
      "1                            Politics and Mental Health   \n",
      "2     Be brutal, what would make you leave your frie...   \n",
      "3     We'll be completely forgotten after a few gene...   \n",
      "4                      My Place Is Right By Your Side üíï   \n",
      "...                                                 ...   \n",
      "7262  I thought i was doing okay, but I feel like I‚Äô...   \n",
      "7263   I've convinced myself that I'm really fucked up.   \n",
      "7264             How to get a \"fix\" without negativity?   \n",
      "7265                             what is wrong with me?   \n",
      "7266  So I saw a psychologist for the first time eve...   \n",
      "\n",
      "                                                   body                author  \\\n",
      "0     Hey r/mentalhealth! We're looking to grow our ...       DrivesInCircles   \n",
      "1     Hello friends!\\n\\nThe team has noticed an incr...                  Pi25   \n",
      "2     I'm sorry if this sounds rude, it's not my int...                unsw4g   \n",
      "3     I am trying to contemplate the meaning of life...  ButterscotchPure6436   \n",
      "4     If you‚Äôre feeling alone, fighting a battle rag...           happy_neets   \n",
      "...                                                 ...                   ...   \n",
      "7262  I‚Äôve been fighting so hard to stay healthy, to...       Ok_Interest2054   \n",
      "7263  So, I've been struggling with these feelings f...               shilcan   \n",
      "7264  I have an addiction to rage-inducing subreddit...       rachaelonreddit   \n",
      "7265  Today I had a breakdown that lasted over an ho...      ActiveWonder3932   \n",
      "7266  So I have PTSD, schrizophernia OCD and Bipolar...      Ill-Estimate4558   \n",
      "\n",
      "                     date  \n",
      "0     2024-07-13 12:25:58  \n",
      "1     2024-07-12 16:32:44  \n",
      "2     2024-10-10 13:12:13  \n",
      "3     2024-10-10 17:52:03  \n",
      "4     2024-10-10 19:10:25  \n",
      "...                   ...  \n",
      "7262  2024-08-23 18:14:02  \n",
      "7263  2024-08-23 07:19:45  \n",
      "7264  2024-08-23 04:16:26  \n",
      "7265  2024-08-23 04:11:05  \n",
      "7266  2024-08-22 16:12:18  \n",
      "\n",
      "[4654 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from prawcore.exceptions import RequestException, ResponseException, BadRequest\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 1000\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'controversial' and time_filter:\n",
    "            return subreddit.controversial(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "        elif sort_method == 'rising':\n",
    "            return subreddit.rising(limit=limit)\n",
    "        elif sort_method == 'random_rising':\n",
    "            return subreddit.random_rising(limit=limit)\n",
    "    except (RequestException, ResponseException, BadRequest) as e:\n",
    "        print(f\"API Error ({e}). Retrying after a 5-minute wait...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "def convert_timestamp(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id  # Add post ID to help identify duplicates\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this week'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='week'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'today'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='day'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 rising posts\n",
    "    for submission in fetch_posts(subreddit, 'rising', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 controversial posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'controversial', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 random rising posts (if supported)\n",
    "    try:\n",
    "        for submission in fetch_posts(subreddit, 'random_rising', batch_size):\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "        show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "    except AttributeError:\n",
    "        print(\"random_rising is not supported on this subreddit.\")\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mentalhealth_ultimate_date.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Temp\\ipykernel_27340\\4250697095.py:47: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 947/1420 posts collected (66.69% complete)\n",
      "Progress: 1929/1420 posts collected (135.85% complete)\n",
      "Progress: 2921/1420 posts collected (205.70% complete)\n",
      "Progress: 3919/1420 posts collected (275.99% complete)\n",
      "Progress: 4914/1420 posts collected (346.06% complete)\n",
      "Progress: 5912/1420 posts collected (416.34% complete)\n",
      "Progress: 6272/1420 posts collected (441.69% complete)\n",
      "Progress: 6297/1420 posts collected (443.45% complete)\n",
      "Progress: 7294/1420 posts collected (513.66% complete)\n",
      "Progress: 7319/1420 posts collected (515.42% complete)\n",
      "                                                  title  \\\n",
      "0     New wiki on how to avoid accidentally encourag...   \n",
      "1     Please remember that NO ACTIVISM of any kind i...   \n",
      "2                    I'm bleeding out on a park bench.    \n",
      "3     Hiding information about how to kill oneself s...   \n",
      "4                                               RIP bro   \n",
      "...                                                 ...   \n",
      "7289  What exactly do they mean when they ask if I \"...   \n",
      "7290  your teenage years will be the \"best time of y...   \n",
      "7291                       Wait till neuralink and such   \n",
      "7292           Desperately want to be with a character.   \n",
      "7293  i feel so horrible i wanna kill myself someone...   \n",
      "\n",
      "                                                   body                author  \\\n",
      "0     We've been seeing a worrying increase in pro-s...              SQLwitch   \n",
      "1     Activism, i.e. advocating or fundraising for s...              SQLwitch   \n",
      "2     There's an ambulance that keeps going up and d...    Rude-Geologist5914   \n",
      "3     I want to kill myself. My life is ruined and I...             MarcJAMBA   \n",
      "4     Previous-Ship-2505 \\nHe jumped from a building...  Able_Philosopher_767   \n",
      "...                                                 ...                   ...   \n",
      "7289  Every single time I've ever called a hotline t...       Hour_Trade_3691   \n",
      "7290  My friend (F13) has diagnosed with BPD, ADHD, ...  bugfrommetamorphosis   \n",
      "7291  Wait till the brain implants combine ai in cou...           No-Gur-7191   \n",
      "7292  More than anything I just want to die and dire...        mediocrecolors   \n",
      "7293  today i came out of the shower and i was apply...  Affectionate-Mail920   \n",
      "\n",
      "                     date  \n",
      "0     2019-09-03 15:49:51  \n",
      "1     2021-09-10 00:01:50  \n",
      "2     2024-10-10 16:45:53  \n",
      "3     2024-10-10 18:03:01  \n",
      "4     2024-10-10 18:18:17  \n",
      "...                   ...  \n",
      "7289  2024-08-11 02:31:17  \n",
      "7290  2024-08-11 00:12:04  \n",
      "7291  2024-08-09 21:53:34  \n",
      "7292  2024-08-09 18:51:39  \n",
      "7293  2024-08-09 07:17:54  \n",
      "\n",
      "[4919 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from prawcore.exceptions import RequestException, ResponseException, BadRequest\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"suicidewatch\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 1000\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'controversial' and time_filter:\n",
    "            return subreddit.controversial(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "        elif sort_method == 'rising':\n",
    "            return subreddit.rising(limit=limit)\n",
    "        elif sort_method == 'random_rising':\n",
    "            return subreddit.random_rising(limit=limit)\n",
    "    except (RequestException, ResponseException, BadRequest) as e:\n",
    "        print(f\"API Error ({e}). Retrying after a 5-minute wait...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "def convert_timestamp(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id  # Add post ID to help identify duplicates\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this week'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='week'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'today'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='day'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 rising posts\n",
    "    for submission in fetch_posts(subreddit, 'rising', batch_size):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 controversial posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'controversial', batch_size, time_filter='year'):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'author': submission.author.name if submission.author else 'deleted',\n",
    "            'date': convert_timestamp(submission.created_utc),\n",
    "            'id': submission.id\n",
    "        })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 random rising posts (if supported)\n",
    "    try:\n",
    "        for submission in fetch_posts(subreddit, 'random_rising', batch_size):\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "        show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "    except AttributeError:\n",
    "        print(\"random_rising is not supported on this subreddit.\")\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('suicidewatch_ultimate_date.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 196/1568 posts collected (12.50% complete)\n",
      "Progress: 395/1568 posts collected (25.19% complete)\n",
      "Progress: 438/1568 posts collected (27.93% complete)\n",
      "Progress: 634/1568 posts collected (40.43% complete)\n",
      "Progress: 842/1568 posts collected (53.70% complete)\n",
      "Progress: 1071/1568 posts collected (68.30% complete)\n",
      "Progress: 1119/1568 posts collected (71.36% complete)\n",
      "Progress: 1126/1568 posts collected (71.81% complete)\n",
      "Progress: 1335/1568 posts collected (85.14% complete)\n",
      "Progress: 1342/1568 posts collected (85.59% complete)\n",
      "Progress: 1538/1568 posts collected (98.09% complete)\n",
      "Progress: 1737/1568 posts collected (110.78% complete)\n",
      "Progress: 1780/1568 posts collected (113.52% complete)\n",
      "Progress: 1976/1568 posts collected (126.02% complete)\n",
      "Progress: 2184/1568 posts collected (139.29% complete)\n",
      "Progress: 2413/1568 posts collected (153.89% complete)\n",
      "Progress: 2465/1568 posts collected (157.21% complete)\n",
      "Progress: 2472/1568 posts collected (157.65% complete)\n",
      "Progress: 2681/1568 posts collected (170.98% complete)\n",
      "Progress: 2688/1568 posts collected (171.43% complete)\n",
      "                                                  title  \\\n",
      "0            Is there a term for obsessive ruminating?    \n",
      "1     Can‚Äôt  stop thinking about how much hurricane ...   \n",
      "2     Why do therapy techniques just make things wor...   \n",
      "3     Got addicted to edging then someone made me fe...   \n",
      "4                          Extreme anxiety in the night   \n",
      "...                                                 ...   \n",
      "1330                                 Drinking confusion   \n",
      "1331  Worried about rabies, Is this bird crap or bat...   \n",
      "1332                            Can I be myself again üò≠   \n",
      "1333                                  Prozac/fluoxetine   \n",
      "1334  Best way to support husband with major depress...   \n",
      "\n",
      "                                                   body                author  \\\n",
      "0     I can‚Äôt find a term. Rumination disorder is a ...    Ancient-Employ3793   \n",
      "1     Hi, I live in Florida, not even two weeks ago ...               sappy__   \n",
      "2     Emotional regulation, grounding techniques etc...     Andromeda_Phoenix   \n",
      "3     Yo this post is kinda weird ngl so just a head...       Agile_Paper3765   \n",
      "4     Please help me. I am very uneasy in the night,...      everydayawkward_   \n",
      "...                                                 ...                   ...   \n",
      "1330  Hey all- this is a quick before bed question t...           zikajuulpod   \n",
      "1331  Is this bird crap or bat crap?(Sorry for askin...  Environmental-Egg-50   \n",
      "1332  Hi everyone, So this is gonna be kind of long ...       Funny_Wait_3459   \n",
      "1333  I am only on day 5 of Fluoxetone 10 mg with th...    Ornery-Option-7609   \n",
      "1334  TL;DR I‚Äôm looking for support/advice on how to...      AlarmedArugula99   \n",
      "\n",
      "             flair  \n",
      "0     Need Support  \n",
      "1     Need Support  \n",
      "2     Need Support  \n",
      "3     Need Support  \n",
      "4     Need Support  \n",
      "...            ...  \n",
      "1330  Need Support  \n",
      "1331  Need Support  \n",
      "1332  Need Support  \n",
      "1333  Need Support  \n",
      "1334  Need Support  \n",
      "\n",
      "[815 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from prawcore.exceptions import RequestException, ResponseException, BadRequest\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 1000\n",
    "desired_flair = \"Need Support\"  # Define the desired flair to filter posts\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'controversial' and time_filter:\n",
    "            return subreddit.controversial(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "        elif sort_method == 'rising':\n",
    "            return subreddit.rising(limit=limit)\n",
    "        elif sort_method == 'random_rising':\n",
    "            return subreddit.random_rising(limit=limit)\n",
    "    except (RequestException, ResponseException, BadRequest) as e:\n",
    "        print(f\"API Error ({e}). Retrying after a 5-minute wait...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        # Check if the post's flair matches the desired flair\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,  # Store the flair for reference\n",
    "                'id': submission.id  # Add post ID to help identify duplicates\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this week'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='week'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'today'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='day'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 rising posts\n",
    "    for submission in fetch_posts(subreddit, 'rising', batch_size):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 controversial posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'controversial', batch_size, time_filter='year'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 random rising posts (if supported)\n",
    "    try:\n",
    "        for submission in fetch_posts(subreddit, 'random_rising', batch_size):\n",
    "            if submission.link_flair_text == desired_flair:\n",
    "                posts.append({\n",
    "                    'title': submission.title,\n",
    "                    'body': submission.selftext,\n",
    "                    'author': submission.author.name if submission.author else 'deleted',\n",
    "                    'flair': submission.link_flair_text,\n",
    "                    'id': submission.id\n",
    "                })\n",
    "        show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "    except AttributeError:\n",
    "        print(\"random_rising is not supported on this subreddit.\")\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mentalhealth_nees_support.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Temp\\ipykernel_27340\\2892366434.py:49: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 196/1511 posts collected (12.97% complete)\n",
      "Progress: 395/1511 posts collected (26.14% complete)\n",
      "Progress: 438/1511 posts collected (28.99% complete)\n",
      "Progress: 634/1511 posts collected (41.96% complete)\n",
      "Progress: 842/1511 posts collected (55.72% complete)\n",
      "Progress: 1075/1511 posts collected (71.14% complete)\n",
      "Progress: 1127/1511 posts collected (74.59% complete)\n",
      "Progress: 1134/1511 posts collected (75.05% complete)\n",
      "Progress: 1343/1511 posts collected (88.88% complete)\n",
      "Progress: 1349/1511 posts collected (89.28% complete)\n",
      "Progress: 1545/1511 posts collected (102.25% complete)\n",
      "Progress: 1744/1511 posts collected (115.42% complete)\n",
      "Progress: 1787/1511 posts collected (118.27% complete)\n",
      "Progress: 1983/1511 posts collected (131.24% complete)\n",
      "Progress: 2191/1511 posts collected (145.00% complete)\n",
      "Progress: 2424/1511 posts collected (160.42% complete)\n",
      "Progress: 2476/1511 posts collected (163.86% complete)\n",
      "Progress: 2482/1511 posts collected (164.26% complete)\n",
      "Progress: 2691/1511 posts collected (178.09% complete)\n",
      "Progress: 2697/1511 posts collected (178.49% complete)\n",
      "                                                  title  \\\n",
      "0            Is there a term for obsessive ruminating?    \n",
      "1     Can‚Äôt  stop thinking about how much hurricane ...   \n",
      "2     Why do therapy techniques just make things wor...   \n",
      "3     Got addicted to edging then someone made me fe...   \n",
      "4                          Extreme anxiety in the night   \n",
      "...                                                 ...   \n",
      "1338                                 Drinking confusion   \n",
      "1339  Worried about rabies, Is this bird crap or bat...   \n",
      "1340                            Can I be myself again üò≠   \n",
      "1341                                  Prozac/fluoxetine   \n",
      "1342  Best way to support husband with major depress...   \n",
      "\n",
      "                                                   body                author  \\\n",
      "0     I can‚Äôt find a term. Rumination disorder is a ...    Ancient-Employ3793   \n",
      "1     Hi, I live in Florida, not even two weeks ago ...               sappy__   \n",
      "2     Emotional regulation, grounding techniques etc...     Andromeda_Phoenix   \n",
      "3     Yo this post is kinda weird ngl so just a head...       Agile_Paper3765   \n",
      "4     Please help me. I am very uneasy in the night,...      everydayawkward_   \n",
      "...                                                 ...                   ...   \n",
      "1338  Hey all- this is a quick before bed question t...           zikajuulpod   \n",
      "1339  Is this bird crap or bat crap?(Sorry for askin...  Environmental-Egg-50   \n",
      "1340  Hi everyone, So this is gonna be kind of long ...       Funny_Wait_3459   \n",
      "1341  I am only on day 5 of Fluoxetone 10 mg with th...    Ornery-Option-7609   \n",
      "1342  TL;DR I‚Äôm looking for support/advice on how to...      AlarmedArugula99   \n",
      "\n",
      "             flair                 date  \n",
      "0     Need Support  2024-10-10 20:07:04  \n",
      "1     Need Support  2024-10-10 23:31:28  \n",
      "2     Need Support  2024-10-10 23:02:31  \n",
      "3     Need Support  2024-10-10 22:08:06  \n",
      "4     Need Support  2024-10-10 21:29:09  \n",
      "...            ...                  ...  \n",
      "1338  Need Support  2024-08-27 02:34:49  \n",
      "1339  Need Support  2024-08-26 22:33:56  \n",
      "1340  Need Support  2024-08-26 00:09:55  \n",
      "1341  Need Support  2024-08-25 22:23:39  \n",
      "1342  Need Support  2024-08-25 14:38:45  \n",
      "\n",
      "[815 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from prawcore.exceptions import RequestException, ResponseException, BadRequest\n",
    "\n",
    "# Initialize the Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=True,\n",
    "    client_id=\"kv4i1eHL9WnbMkNyS58xVw\",\n",
    "    client_secret=\"G6LzULvIFr_l3kC4yT80BlQ0Bim6rg\",\n",
    "    username=\"wucanyue1\",\n",
    "    password='Yhx2043789012@'\n",
    ")\n",
    "\n",
    "# Fetch posts from different sorting categories and timeframes\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "\n",
    "# List to store post data\n",
    "posts = []\n",
    "\n",
    "# Set target post count between 1400 and 1600\n",
    "desired_post_count = random.randint(1400, 1600)\n",
    "batch_size = 1000\n",
    "desired_flair = \"Need Support\"  # Define the desired flair to filter posts\n",
    "\n",
    "def fetch_posts(subreddit, sort_method, limit, time_filter=None):\n",
    "    try:\n",
    "        if sort_method == 'top' and time_filter:\n",
    "            return subreddit.top(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'controversial' and time_filter:\n",
    "            return subreddit.controversial(time_filter=time_filter, limit=limit)\n",
    "        elif sort_method == 'hot':\n",
    "            return subreddit.hot(limit=limit)\n",
    "        elif sort_method == 'new':\n",
    "            return subreddit.new(limit=limit)\n",
    "        elif sort_method == 'rising':\n",
    "            return subreddit.rising(limit=limit)\n",
    "        elif sort_method == 'random_rising':\n",
    "            return subreddit.random_rising(limit=limit)\n",
    "    except (RequestException, ResponseException, BadRequest) as e:\n",
    "        print(f\"API Error ({e}). Retrying after a 5-minute wait...\")\n",
    "        time.sleep(300)  # Wait for 300 seconds (5 minutes) before retrying\n",
    "        return fetch_posts(subreddit, sort_method, limit, time_filter)\n",
    "\n",
    "# Convert Unix timestamp to readable date format\n",
    "def convert_timestamp(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Function to show progress\n",
    "def show_progress(collected_posts, target_posts):\n",
    "    percentage = (collected_posts / target_posts) * 100\n",
    "    print(f\"Progress: {collected_posts}/{target_posts} posts collected ({percentage:.2f}% complete)\")\n",
    "\n",
    "# Fetch posts in batches until we reach the target range\n",
    "while len(posts) < desired_post_count:\n",
    "    # Fetch a batch of 100 posts from 'hot'\n",
    "    for submission in fetch_posts(subreddit, 'hot', batch_size):\n",
    "        # Check if the post's flair matches the desired flair\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,  # Store the flair for reference\n",
    "                'date': convert_timestamp(submission.created_utc),  # Convert and store the date\n",
    "                'id': submission.id  # Add post ID to help identify duplicates\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 posts from 'new'\n",
    "    for submission in fetch_posts(subreddit, 'new', batch_size):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'all time'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='all'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='year'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this month'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='month'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'this week'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='week'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 top posts from 'today'\n",
    "    for submission in fetch_posts(subreddit, 'top', batch_size, time_filter='day'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 rising posts\n",
    "    for submission in fetch_posts(subreddit, 'rising', batch_size):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 controversial posts from 'this year'\n",
    "    for submission in fetch_posts(subreddit, 'controversial', batch_size, time_filter='year'):\n",
    "        if submission.link_flair_text == desired_flair:\n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'flair': submission.link_flair_text,\n",
    "                'date': convert_timestamp(submission.created_utc),\n",
    "                'id': submission.id\n",
    "            })\n",
    "    show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "\n",
    "    # Fetch a batch of 100 random rising posts (if supported)\n",
    "    try:\n",
    "        for submission in fetch_posts(subreddit, 'random_rising', batch_size):\n",
    "            if submission.link_flair_text == desired_flair:\n",
    "                posts.append({\n",
    "                    'title': submission.title,\n",
    "                    'body': submission.selftext,\n",
    "                    'author': submission.author.name if submission.author else 'deleted',\n",
    "                    'flair': submission.link_flair_text,\n",
    "                    'date': convert_timestamp(submission.created_utc),\n",
    "                    'id': submission.id\n",
    "                })\n",
    "        show_progress(len(posts), desired_post_count)  # Show progress after fetching a batch\n",
    "    except AttributeError:\n",
    "        print(\"random_rising is not supported on this subreddit.\")\n",
    "\n",
    "# Only remove duplicates once we have collected enough posts\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Remove duplicate posts based on the 'id' column\n",
    "df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Drop the 'id' column since it's no longer needed\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mentalhealth_need_support_ultimate.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('mentalhealth4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Local\\Temp\\ipykernel_14992\\3035103538.py:22: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete and saved to reddit_suicidewatch_posts.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime  # To convert Unix timestamp to a readable date\n",
    "\n",
    "# Assuming you have already authenticated with Reddit using PRAW\n",
    "# Example authentication (if not already done):\n",
    "# reddit = praw.Reddit(client_id=\"YOUR_CLIENT_ID\",\n",
    "#                      client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "#                      username=\"YOUR_REDDIT_USERNAME\",\n",
    "#                      password=\"YOUR_REDDIT_PASSWORD\",\n",
    "#                      user_agent=\"YOUR_USER_AGENT\")\n",
    "\n",
    "subreddit = reddit.subreddit(\"SuicideWatch\")\n",
    "comments = []\n",
    "\n",
    "# Loop until you gather data from posts\n",
    "while len(comments) < 5000:\n",
    "    for submission in subreddit.hot(limit=100):\n",
    "        try:\n",
    "            # Convert the post creation time from Unix timestamp to human-readable format\n",
    "            post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Append the relevant information to the comments list\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext,  # Post content (selftext)\n",
    "                'user': submission.author.name if submission.author else '[deleted]',  # Handle deleted authors\n",
    "                'date': post_date  # Post creation date\n",
    "            })\n",
    "\n",
    "            if len(comments) >= 5000:  # Stop once we have 5000 records\n",
    "                break\n",
    "\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            if e.response.status_code == 429:  # Handle TooManyRequests error\n",
    "                print(\"Rate limit exceeded. Sleeping for 60 seconds...\")\n",
    "                time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "                break  # Break the inner loop to retry fetching submissions\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")  # Handle other exceptions\n",
    "        except AttributeError:  # Handle case when author is deleted or unavailable\n",
    "            post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext, \n",
    "                'user': '[deleted]',  # Assign '[deleted]' for unavailable authors\n",
    "                'date': post_date  # Post creation date\n",
    "            })\n",
    "    time.sleep(2)  # Add a delay of 2 seconds between requests\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(comments)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('reddit_suicidewatch_posts.csv', index=False)\n",
    "\n",
    "print(\"Data collection complete and saved to reddit_suicidewatch_posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime  # To convert Unix timestamp to a readable date\n",
    "\n",
    "# Assuming you have already authenticated with Reddit using PRAW\n",
    "# Example authentication (if not already done):\n",
    "# reddit = praw.Reddit(client_id=\"YOUR_CLIENT_ID\",\n",
    "#                      client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "#                      username=\"YOUR_REDDIT_USERNAME\",\n",
    "#                      password=\"YOUR_REDDIT_PASSWORD\",\n",
    "#                      user_agent=\"YOUR_USER_AGENT\")\n",
    "\n",
    "# Subreddit 'mentalhealth' and flair 'Need Support' URL-specific posts\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "comments = []\n",
    "\n",
    "# Loop until you gather data from posts with the flair \"Need Support\"\n",
    "while len(comments) < 5000:\n",
    "    for submission in subreddit.search('flair:\"Need Support\"', sort='hot', limit=100):\n",
    "        try:\n",
    "            # Convert the post creation time from Unix timestamp to human-readable format\n",
    "            post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext,  # Post content (selftext)\n",
    "                'user': submission.author.name if submission.author else '[deleted]',  # Handle deleted authors\n",
    "                'date': post_date  # Post creation date\n",
    "            })\n",
    "\n",
    "            if len(comments) >= 5000:  # Stop once we have 5000 records\n",
    "                break\n",
    "\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            if e.response.status_code == 429:  # Handle TooManyRequests error\n",
    "                print(\"Rate limit exceeded. Sleeping for 60 seconds...\")\n",
    "                time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "                break  # Break the inner loop to retry fetching submissions\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")  # Handle other exceptions\n",
    "        except AttributeError:  # Handle case when author is deleted or unavailable\n",
    "            post_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext, \n",
    "                'user': '[deleted]',  # Assign '[deleted]' for unavailable authors\n",
    "                'date': post_date  # Post creation date\n",
    "            })\n",
    "    time.sleep(2)  # Add a delay of 2 seconds between requests\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(comments)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('reddit_need_support_posts_filtered.csv', index=False)\n",
    "\n",
    "print(\"Data collection complete and saved to reddit_need_support_posts_filtered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseException",
     "evalue": "received 401 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Loop until you gather data from 1000 posts\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(comments) \u001b[38;5;241m<\u001b[39m target_post_count:\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fetch top 100 posts in 'hot' category\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcomments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselftext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Post content (selftext)\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[deleted]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Handle deleted authors\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\models\\listing\\generator.py:63\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing):\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\models\\listing\\generator.py:89\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_sublist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\reddit.py:712\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;129m@_deprecate_args\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    704\u001b[0m     params: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    705\u001b[0m ):\n\u001b[0;32m    706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objectify_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objector\u001b[38;5;241m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m retry_strategy_state\u001b[38;5;241m.\u001b[39msleep()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m do_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munauthorized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m             response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\rate_limit.py:46\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Rate limit the call to ``request_function``.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m:param request_function: A function call that returns an HTTP response object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay()\n\u001b[1;32m---> 46\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mset_header_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m response \u001b[38;5;241m=\u001b[39m request_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\sessions.py:282\u001b[0m, in \u001b[0;36mSession._set_header_callback\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_header_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authorizer\u001b[38;5;241m.\u001b[39mis_valid() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authorizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 282\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authorizer\u001b[38;5;241m.\u001b[39maccess_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\auth.py:378\u001b[0m, in \u001b[0;36mReadOnlyAuthorizer.refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scopes:\n\u001b[0;32m    377\u001b[0m     additional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scopes)\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient_credentials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\auth.py:155\u001b[0m, in \u001b[0;36mBaseAuthorizer._request_token\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    153\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39mreddit_url \u001b[38;5;241m+\u001b[39m const\u001b[38;5;241m.\u001b[39mACCESS_TOKEN_PATH\n\u001b[0;32m    154\u001b[0m pre_request_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 155\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authenticator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m payload \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:  \u001b[38;5;66;03m# Why are these OKAY responses?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86153\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prawcore\\auth.py:59\u001b[0m, in \u001b[0;36mBaseAuthenticator._post\u001b[1;34m(self, url, success_status, **data)\u001b[0m\n\u001b[0;32m     51\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m     url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m success_status:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseException(response)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseException\u001b[0m: received 401 HTTP response"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize PRAW Reddit object (make sure to authenticate appropriately)\n",
    "# Example: reddit = praw.Reddit(client_id='YOUR_CLIENT_ID', client_secret='YOUR_CLIENT_SECRET', user_agent='YOUR_USER_AGENT')\n",
    "\n",
    "subreddit = reddit.subreddit(\"mentalhealth\")\n",
    "comments = []\n",
    "target_post_count = 1000  # Set the target number of posts to scrape\n",
    "\n",
    "# Loop until you gather data from 1000 posts\n",
    "while len(comments) < target_post_count:\n",
    "    for submission in subreddit.hot(limit=100):  # Fetch top 100 posts in 'hot' category\n",
    "        try:\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext,  # Post content (selftext)\n",
    "                'user': submission.author.name if submission.author else '[deleted]'  # Handle deleted authors\n",
    "            })\n",
    "\n",
    "            # Notify progress after each post is scraped\n",
    "            print(f\"Scraped post {len(comments)}: {submission.title}\")\n",
    "\n",
    "            if len(comments) >= target_post_count:  # Stop once we have 1000 records\n",
    "                break\n",
    "        except praw.exceptions.PRAWException as e:\n",
    "            if e.response.status_code == 429:  # Handle TooManyRequests error\n",
    "                print(\"Rate limit exceeded. Sleeping for 60 seconds...\")\n",
    "                time.sleep(60)  # Wait for 60 seconds before retrying\n",
    "                break  # Break the inner loop to retry fetching submissions\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")  # Handle other exceptions\n",
    "        except AttributeError:  # Handle case when author is deleted or unavailable\n",
    "            comments.append({\n",
    "                'title': submission.title, \n",
    "                'content': submission.selftext, \n",
    "                'user': '[deleted]'  # Assign '[deleted]' for unavailable authors\n",
    "            })\n",
    "            print(f\"Scraped post {len(comments)}: {submission.title} (Author: [deleted])\")\n",
    "            \n",
    "    time.sleep(2)  # Add a delay of 2 seconds between requests\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(comments)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('reddit_posts.csv', index=False)\n",
    "\n",
    "print(\"Data collection complete and saved to reddit_posts.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
